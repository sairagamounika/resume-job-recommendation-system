{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e8498ff-510b-4b9d-a7f2-a40596f773cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROC_DIR: D:\\Projects\\ResumeJobRecommender\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Notebook 2 — Resume TF-IDF\n",
    "# ==============================\n",
    "\n",
    "BASE_DIR = r\"D:\\Projects\\ResumeJobRecommender\"\n",
    "RAW_DIR  = BASE_DIR + r\"\\data\\raw\"\n",
    "PROC_DIR = BASE_DIR + r\"\\data\\processed\"\n",
    "\n",
    "import os, re, json, unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "os.makedirs(PROC_DIR, exist_ok=True)\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "\n",
    "print(\"PROC_DIR:\", PROC_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b74894c8-cbca-41eb-813f-71ccc946e342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: (2158, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resume_id</th>\n",
       "      <th>category</th>\n",
       "      <th>text_raw</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PDF_000001</td>\n",
       "      <td>ACCOUNTANT</td>\n",
       "      <td>ACCOUNTANT\\nSummary\\nAccountant for a Medium sized Company\\nExperience\\n01/2009 to Current\\nAccountant Company Name ï¼​ City , State\\nHired by their CPA firm to handle all accounting and job cost ...</td>\n",
       "      <td>accountant summary accountant for a medium sized company experience 01 2009 to current accountant company name ï1 4 city state hired by their cpa firm to handle all accounting and job cost reporti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PDF_000002</td>\n",
       "      <td>ACCOUNTANT</td>\n",
       "      <td>ACCOUNTANT\\nInterests\\nBuffalo Creek Golf Club, Rockwall, TX May 2012-August 2012 *Maintain golf carts and driving range\\nExperience\\n03/2016 to 03/2018\\nAccountant Company Name ï¼​ City , State\\n...</td>\n",
       "      <td>accountant interests buffalo creek golf club rockwall tx may 2012 august 2012 maintain golf carts and driving range experience 03 2016 to 03 2018 accountant company name ï1 4 city state reconcile ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PDF_000003</td>\n",
       "      <td>ACCOUNTANT</td>\n",
       "      <td>ACCOUNTANT\\nSummary\\nIf you need someone who delivers sharp results, I can help. Well qualified and results oriented Accounting Professional with over fourteen years of\\nsuccessful experience in p...</td>\n",
       "      <td>accountant summary if you need someone who delivers sharp results i can help well qualified and results oriented accounting professional with over fourteen years of successful experience in positi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    resume_id    category  \\\n",
       "0  PDF_000001  ACCOUNTANT   \n",
       "1  PDF_000002  ACCOUNTANT   \n",
       "2  PDF_000003  ACCOUNTANT   \n",
       "\n",
       "                                                                                                                                                                                                  text_raw  \\\n",
       "0  ACCOUNTANT\\nSummary\\nAccountant for a Medium sized Company\\nExperience\\n01/2009 to Current\\nAccountant Company Name ï¼​ City , State\\nHired by their CPA firm to handle all accounting and job cost ...   \n",
       "1  ACCOUNTANT\\nInterests\\nBuffalo Creek Golf Club, Rockwall, TX May 2012-August 2012 *Maintain golf carts and driving range\\nExperience\\n03/2016 to 03/2018\\nAccountant Company Name ï¼​ City , State\\n...   \n",
       "2  ACCOUNTANT\\nSummary\\nIf you need someone who delivers sharp results, I can help. Well qualified and results oriented Accounting Professional with over fourteen years of\\nsuccessful experience in p...   \n",
       "\n",
       "                                                                                                                                                                                                text_clean  \n",
       "0  accountant summary accountant for a medium sized company experience 01 2009 to current accountant company name ï1 4 city state hired by their cpa firm to handle all accounting and job cost reporti...  \n",
       "1  accountant interests buffalo creek golf club rockwall tx may 2012 august 2012 maintain golf carts and driving range experience 03 2016 to 03 2018 accountant company name ï1 4 city state reconcile ...  \n",
       "2  accountant summary if you need someone who delivers sharp results i can help well qualified and results oriented accounting professional with over fourteen years of successful experience in positi...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resumes = pd.read_csv(PROC_DIR + r\"\\resume_cleaned.csv\")\n",
    "print(\"Loaded:\", resumes.shape)\n",
    "resumes.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5242194-c95f-4d81-833b-16fb60797bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resume_id</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PDF_000001</td>\n",
       "      <td>Accountant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PDF_000002</td>\n",
       "      <td>Accountant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PDF_000003</td>\n",
       "      <td>Accountant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PDF_000004</td>\n",
       "      <td>Accountant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PDF_000005</td>\n",
       "      <td>Accountant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    resume_id    category\n",
       "0  PDF_000001  Accountant\n",
       "1  PDF_000002  Accountant\n",
       "2  PDF_000003  Accountant\n",
       "3  PDF_000004  Accountant\n",
       "4  PDF_000005  Accountant"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==============================\n",
    "# STEP 2 — Clean Category Labels (IN PLACE)\n",
    "# ==============================\n",
    "# Make categories readable and consistent for grouping/EDA.\n",
    "# (We do not keep a second column; we just normalize in place.)\n",
    "\n",
    "resumes[\"category\"] = (\n",
    "    resumes[\"category\"]\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .str.replace(r\"[_\\-]+\", \" \", regex=True)   # \"_\" or \"-\" -> space\n",
    "    .str.replace(r\"\\s+\", \" \", regex=True)      # collapse spaces\n",
    "    .str.title()                                # Title Case\n",
    "    .str.replace(r\"\\bHr\\b\", \"HR\", regex=True)   # fix common acronyms\n",
    "    .str.replace(r\"\\bIt\\b\", \"IT\", regex=True)\n",
    ")\n",
    "\n",
    "# Quick peek\n",
    "resumes[[\"resume_id\",\"category\"]].head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9358d2c-a8fa-4eaf-bcfa-de0ca17fcdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact duplicate rows (by text_clean): 797\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# STEP 3 — (Optional) Exact Dedup by Cleaned Text\n",
    "# ==============================\n",
    "# Many IT resumes share boilerplate; exact duplicates may exist by text_clean.\n",
    "# We only *report* them; set DO_DEDUP=True to actually drop them.\n",
    "\n",
    "dup_mask = resumes.duplicated(subset=[\"text_clean\"], keep=\"first\")\n",
    "print(\"Exact duplicate rows (by text_clean):\", int(dup_mask.sum()))\n",
    "\n",
    "DO_DEDUP = False  # <= set to True if you want to drop exact duplicates\n",
    "if DO_DEDUP:\n",
    "    resumes = resumes.loc[~dup_mask].reset_index(drop=True)\n",
    "    print(\"After dedup:\", resumes.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e99d189-296d-484d-b90c-79b9fece2b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\saira\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed text preview:\n",
      "Saved: D:\\Projects\\ResumeJobRecommender\\data\\processed\\resumes_with_processed_text.csv\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# STEP 4 — Stopword Removal -> processed_text\n",
    "# ==============================\n",
    "# We remove common English stopwords to emphasize skills/role terms.\n",
    "# (TF-IDF also downweights frequent words, but this helps sharpen vocabulary.)\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "STOP = set(stopwords.words(\"english\"))\n",
    "\n",
    "proc_texts = []\n",
    "for s in resumes[\"text_clean\"].astype(str).tolist():\n",
    "    words = [w for w in s.split() if w not in STOP]\n",
    "    proc_texts.append(\" \".join(words))\n",
    "\n",
    "resumes[\"processed_text\"] = proc_texts\n",
    "print(\"Processed text preview:\")\n",
    "resumes[[\"resume_id\",\"category\",\"processed_text\"]].head(3)\n",
    "\n",
    "# Save checkpoint with processed_text\n",
    "resumes.to_csv(PROC_DIR + r\"\\resumes_with_processed_text.csv\", index=False)\n",
    "print(\"Saved:\", PROC_DIR + r\"\\resumes_with_processed_text.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b54c0f9-b90b-42f3-b86d-ea5179de9d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (2158, 50000)\n",
      "Saved: D:\\Projects\\ResumeJobRecommender\\data\\processed\\resume_tfidf_matrix.npz\n",
      "Saved: D:\\Projects\\ResumeJobRecommender\\data\\processed\\resume_tfidf_vocab.json\n",
      "Saved: D:\\Projects\\ResumeJobRecommender\\data\\processed\\resume_tfidf_idf.npy\n",
      "Saved: D:\\Projects\\ResumeJobRecommender\\data\\processed\\resume_tfidf_terms.csv\n",
      "Saved: D:\\Projects\\ResumeJobRecommender\\data\\processed\\resume_tfidf_rowindex.csv\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# STEP 5 — TF-IDF Vectorization (unigram + bigram)\n",
    "# ==============================\n",
    "# WHAT: Convert resumes into a sparse TF-IDF matrix (rows = resumes, cols = terms).\n",
    "# WHY: This is the numeric representation we'll later compare with job postings via cosine similarity.\n",
    "# NOTES:\n",
    "\n",
    "#  - We fit on resume text now, and we will use the SAME vocabulary & IDF to vectorize jobs later.\n",
    "#  - We save only the essential artifacts you actually need downstream.\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import sparse\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---- 5.1 Configure the vectorizer\n",
    "# ngram_range=(1,2): capture single words and 2-word phrases (\"data science\", \"project management\")\n",
    "# min_df=3: drop super-rare tokens (noise)\n",
    "# max_df=0.90: drop overly common tokens (boilerplate)\n",
    "# max_features=50k: cap vocabulary to keep memory/speed reasonable\n",
    "tfidf = TfidfVectorizer(\n",
    "    lowercase=False,          # already lowercased in text_clean\n",
    "    analyzer=\"word\",\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=3,\n",
    "    max_df=0.90,\n",
    "    max_features=50000\n",
    ")\n",
    "\n",
    "# ---- 5.2 Fit on processed_text and build the sparse matrix\n",
    "X = tfidf.fit_transform(resumes[\"processed_text\"].astype(str).tolist())\n",
    "print(\"TF-IDF matrix shape:\", X.shape)  # rows = resumes, cols = terms\n",
    "\n",
    "# ---- 5.3 Persist ESSENTIAL artifacts\n",
    "# (a) The sparse matrix: used when computing similarities later\n",
    "sparse.save_npz(PROC_DIR + r\"\\resume_tfidf_matrix.npz\", X)\n",
    "print(\"Saved:\", PROC_DIR + r\"\\resume_tfidf_matrix.npz\")\n",
    "\n",
    "# (b) Vocabulary (term -> column index)\n",
    "#     IMPORTANT: cast NumPy int32 to Python int so JSON can serialize it.\n",
    "vocab_np = tfidf.vocabulary_                 # dict: term -> numpy.int32\n",
    "vocab_py = {term: int(idx) for term, idx in vocab_np.items()}   # <-- CAST HERE\n",
    "\n",
    "with open(PROC_DIR + r\"\\resume_tfidf_vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(vocab_py, f)                   # now JSON-serializable\n",
    "print(\"Saved:\", PROC_DIR + r\"\\resume_tfidf_vocab.json\")\n",
    "\n",
    "# (c) IDF weights (float array). We'll reload these to rebuild the same vectorizer for jobs.\n",
    "np.save(PROC_DIR + r\"\\resume_tfidf_idf.npy\", tfidf.idf_)\n",
    "print(\"Saved:\", PROC_DIR + r\"\\resume_tfidf_idf.npy\")\n",
    "\n",
    "# (d) Ordered term list — human-readable and handy for debugging/inspection\n",
    "terms = tfidf.get_feature_names_out().tolist()\n",
    "pd.DataFrame({\"term\": terms}).to_csv(PROC_DIR + r\"\\resume_tfidf_terms.csv\", index=False)\n",
    "print(\"Saved:\", PROC_DIR + r\"\\resume_tfidf_terms.csv\")\n",
    "\n",
    "# (e) Row index mapping — links resume_id to matrix row index (critical for joins later)\n",
    "resumes[[\"resume_id\"]].assign(row_idx=np.arange(len(resumes))).to_csv(\n",
    "    PROC_DIR + r\"\\resume_tfidf_rowindex.csv\", index=False\n",
    ")\n",
    "print(\"Saved:\", PROC_DIR + r\"\\resume_tfidf_rowindex.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24f8be1-2ae4-4e37-a85a-ff4218aca0aa",
   "metadata": {},
   "source": [
    "#### Quick EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a039d89-cb2c-4d2e-8818-13899f4ef996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (2158, 50000)\n",
      "nonzeros: 967174\n",
      "density (should be tiny): 0.008963614457831325\n",
      "avg nonzero features per resume: 448.1807228915663\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X = sparse.load_npz(r\"D:\\Projects\\ResumeJobRecommender\\data\\processed\\resume_tfidf_matrix.npz\")\n",
    "print(\"shape:\", X.shape)\n",
    "\n",
    "nnz = X.nnz\n",
    "total = X.shape[0] * X.shape[1]\n",
    "density = nnz / total\n",
    "print(\"nonzeros:\", nnz)\n",
    "print(\"density (should be tiny):\", density)\n",
    "print(\"avg nonzero features per resume:\", nnz / X.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a14ad02-f4c7-48e4-8af4-9cfcf597bc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 20 terms: ['00', '00 gpa', '00 per', '000', '000 00', '000 000', '000 additional', '000 annual', '000 annually', '000 employees', '000 lending', '000 million', '000 month', '000 monthly', '000 new', '000 people', '000 per', '000 platinum', '000 revenue', '000 sq']\n",
      "sample vocab lookups: {'accountant': 2050, 'summary': 43686, 'medium': 27763, 'sized': 41263, 'experience': 17339, '01': 23, '2009': 927, 'current': 11286, 'name': 29244, 'ï1': 49985}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "terms = pd.read_csv(r\"D:\\Projects\\ResumeJobRecommender\\data\\processed\\resume_tfidf_terms.csv\")[\"term\"].tolist()\n",
    "with open(r\"D:\\Projects\\ResumeJobRecommender\\data\\processed\\resume_tfidf_vocab.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "print(\"first 20 terms:\", terms[:20])\n",
    "print(\"sample vocab lookups:\", {k:vocab[k] for k in list(vocab)[:10]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0a24ec1-7788-464b-80ca-8d82fdaf273e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resume row 1874 top terms:\n",
      "configuration, network, cisco, servers, cisco router, server, r2, router, dhcp, switches\n",
      "\n",
      "Resume row 1081 top terms:\n",
      "sales, selling, ohio, sold, client list, billing, kentucky, territories, territory, inc\n",
      "\n",
      "Resume row 1027 top terms:\n",
      "social media, media, hostess, social, pr, name city, managed inventory, hootsuite, media platforms, write\n"
     ]
    }
   ],
   "source": [
    "# pick a few random rows and show their strongest TF-IDF tokens\n",
    "rng_rows = np.random.choice(X.shape[0], size=3, replace=False)\n",
    "terms_arr = np.array(terms)\n",
    "\n",
    "for i in rng_rows:\n",
    "    row = X.getrow(i)\n",
    "    idx, data = row.indices, row.data\n",
    "    order = np.argsort(data)[::-1]\n",
    "    top_idx = idx[order][:10]\n",
    "    top_terms = terms_arr[top_idx]\n",
    "    print(f\"\\nResume row {i} top terms:\")\n",
    "    print(\", \".join(top_terms))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df132cc2-940f-4ce4-a09e-d3dcd1e5cf76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    resume_id  row_idx\n",
      "0  PDF_000001        0\n",
      "1  PDF_000002        1\n",
      "2  PDF_000003        2\n",
      "3  PDF_000004        3\n",
      "4  PDF_000005        4\n",
      "unique rows vs. matrix rows: 2158 vs 2158\n"
     ]
    }
   ],
   "source": [
    "map_df = pd.read_csv(r\"D:\\Projects\\ResumeJobRecommender\\data\\processed\\resume_tfidf_rowindex.csv\")\n",
    "print(map_df.head(5))\n",
    "print(\"unique rows vs. matrix rows:\", map_df[\"row_idx\"].nunique(), \"vs\", X.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a7df2eb-97c3-4e04-a4c9-e59d0c8a7c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 frequent terms overall:\n",
      "skills 1715\n",
      "management 1546\n",
      "university 1458\n",
      "team 1451\n",
      "experience 1427\n",
      "state 1349\n",
      "name 1349\n",
      "new 1271\n",
      "project 1256\n",
      "business 1238\n",
      "company name 1208\n",
      "work 1206\n",
      "development 1176\n",
      "city 1171\n",
      "skill 1145\n",
      "city state 1125\n",
      "system 1104\n",
      "customer 1089\n",
      "details 1065\n",
      "information 1062\n"
     ]
    }
   ],
   "source": [
    "# approximate global frequency by counting non-zeros per column\n",
    "col_nnz = np.diff(X.tocsc().indptr)  # fast nnz per column\n",
    "top_cols = np.argsort(col_nnz)[::-1][:20]\n",
    "print(\"Top 20 frequent terms overall:\")\n",
    "for j in top_cols:\n",
    "    print(terms[j], int(col_nnz[j]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49119c76-ce7c-4cba-aebb-4fc70e57e9e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
