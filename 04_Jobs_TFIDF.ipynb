{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "135bec3c-600e-4eb9-9dbf-0fc7499e8c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# NOTEBOOK 4 — Job TF-IDF Representation\n",
    "# ==============================\n",
    "# GOAL:\n",
    "# Transform cleaned job descriptions into TF-IDF vectors\n",
    "# using the same vocabulary and IDF values learned from resumes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "456eb35a-ca85-4c08-b967-3f6488f7d0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROC_DIR: D:\\Projects\\ResumeJobRecommender\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# STEP 0 — Setup & Imports\n",
    "# ==============================\n",
    "BASE_DIR = r\"D:\\Projects\\ResumeJobRecommender\"\n",
    "PROC_DIR = BASE_DIR + r\"\\data\\processed\"\n",
    "\n",
    "import os, json, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 150)\n",
    "print(\"PROC_DIR:\", PROC_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7fc2c5a-d566-47d7-bd5e-c15df513f302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jobs loaded: (15851, 15)\n",
      "['job_id', 'job_title', 'job_description', 'location', 'experience_level', 'work_type', 'min_salary', 'max_salary', 'pay_period', 'currency', 'remote_allowed', 'sponsored', 'job_posting_url', 'job_title_clean', 'job_description_clean']\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# STEP 1 — Load cleaned job postings\n",
    "# ==============================\n",
    "job_path = PROC_DIR + r\"\\job_postings_cleaned.csv\"\n",
    "job_df = pd.read_csv(job_path)\n",
    "print(\"Jobs loaded:\", job_df.shape)\n",
    "print(job_df.columns.tolist())\n",
    "\n",
    "# keep only required text columns\n",
    "job_texts = job_df[\"job_description_clean\"].astype(str).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c18e062-f714-47a4-a91d-d2079feba003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vocab terms: 50000\n",
      "Loaded IDF shape: (50000,)\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# STEP 2 — Load resume TF-IDF vocab + IDF\n",
    "# ==============================\n",
    "vocab_path = PROC_DIR + r\"\\resume_tfidf_vocab.json\"\n",
    "idf_path   = PROC_DIR + r\"\\resume_tfidf_idf.npy\"\n",
    "\n",
    "with open(vocab_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "idf_values = np.load(idf_path)\n",
    "\n",
    "print(\"Loaded vocab terms:\", len(vocab))\n",
    "print(\"Loaded IDF shape:\", idf_values.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b60b70bd-161c-4c2e-8b34-75f4c5c39e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# STEP 3 — Light token cleanup & stopword removal\n",
    "# ==============================\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text(s):\n",
    "    s = str(s).lower()\n",
    "    s = re.sub(r\"[^\\w\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = \" \".join([w for w in s.split() if w not in stop_words])\n",
    "    return s.strip()\n",
    "\n",
    "job_df[\"job_text_processed\"] = job_df[\"job_description_clean\"].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44f4b5ad-594e-4f13-aa55-8cb8c24c33a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job TF-IDF shape: (15851, 50000)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================\n",
    "# STEP 4 — Build TF-IDF for jobs using fixed resume vocab/IDF (version-safe)\n",
    "# ==============================\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from scipy.sparse import spdiags\n",
    "\n",
    "# 1) Count vectors with the SAME vocab (and n-gram setting used for resumes)\n",
    "count_vect = CountVectorizer(\n",
    "    vocabulary=vocab,       # fixed mapping term -> column index (from resumes)\n",
    "    lowercase=False,        # we already lowercased\n",
    "    analyzer=\"word\",\n",
    "    ngram_range=(1, 2)      # match resume vectorizer setting\n",
    ")\n",
    "job_counts = count_vect.transform(job_df[\"job_text_processed\"])\n",
    "\n",
    "# 2) Apply the SAME IDF weights learned on resumes\n",
    "tfidf_transformer = TfidfTransformer(norm=\"l2\", use_idf=True, smooth_idf=False, sublinear_tf=False)\n",
    "tfidf_transformer.idf_ = idf_values\n",
    "# important for newer sklearn: build the internal diagonal\n",
    "tfidf_transformer._idf_diag = spdiags(tfidf_transformer.idf_, diags=0,\n",
    "                                      m=len(tfidf_transformer.idf_), n=len(tfidf_transformer.idf_))\n",
    "\n",
    "job_tfidf = tfidf_transformer.transform(job_counts)\n",
    "print(\"Job TF-IDF shape:\", job_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7d8ca41-72d1-4766-aab4-d6030737b4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nonzeros: 4221263\n",
      "Density: 0.005326\n",
      "Avg nonzero features per job: 266.31\n",
      "First 20 terms: ['00' '00 gpa' '00 per' '000' '000 00' '000 000' '000 additional'\n",
      " '000 annual' '000 annually' '000 employees' '000 lending' '000 million'\n",
      " '000 month' '000 monthly' '000 new' '000 people' '000 per' '000 platinum'\n",
      " '000 revenue' '000 sq']\n",
      "\n",
      "Job row 6060 top terms:\n",
      "['procurement', 'head', 'function', 'acquisitions', 'margins', 'must', 'across', 'supplier', 'expected', 'enable']\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# STEP 5 — QA: Sparsity & content checks\n",
    "# ==============================\n",
    "nonzeros = job_tfidf.nnz\n",
    "density = nonzeros / np.prod(job_tfidf.shape)\n",
    "avg_feats = nonzeros / job_tfidf.shape[0]\n",
    "\n",
    "print(f\"Nonzeros: {nonzeros}\")\n",
    "print(f\"Density: {density:.6f}\")\n",
    "print(f\"Avg nonzero features per job: {avg_feats:.2f}\")\n",
    "\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "print(\"First 20 terms:\", terms[:20])\n",
    "\n",
    "# example: show top 10 terms for one random job\n",
    "row_idx = np.random.randint(0, job_tfidf.shape[0])\n",
    "row_vector = job_tfidf.getrow(row_idx).toarray().ravel()\n",
    "top_indices = row_vector.argsort()[-10:][::-1]\n",
    "print(f\"\\nJob row {row_idx} top terms:\")\n",
    "print([terms[i] for i in top_indices])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e22a031-a1d6-4944-b51a-ea28584be76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Projects\\ResumeJobRecommender\\data\\processed\\job_tfidf_matrix.npz\n",
      "Saved: D:\\Projects\\ResumeJobRecommender\\data\\processed\\job_tfidf_rowindex.csv\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# STEP 6 — Save TF-IDF artifacts\n",
    "# ==============================\n",
    "# matrix\n",
    "matrix_path = PROC_DIR + r\"\\job_tfidf_matrix.npz\"\n",
    "sparse.save_npz(matrix_path, job_tfidf)\n",
    "\n",
    "# row index map\n",
    "rowindex_path = PROC_DIR + r\"\\job_tfidf_rowindex.csv\"\n",
    "job_df[[\"job_id\"]].reset_index(drop=True).reset_index().rename(\n",
    "    columns={\"index\":\"row_idx\"}\n",
    ").to_csv(rowindex_path, index=False)\n",
    "\n",
    "print(\"Saved:\", matrix_path)\n",
    "print(\"Saved:\", rowindex_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db01188f-a41f-4ff7-a7eb-d3f88e066148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 50000\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sp, numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "X_res = sparse.load_npz(r\"D:\\Projects\\ResumeJobRecommender\\data\\processed\\resume_tfidf_matrix.npz\")\n",
    "print(X_res.shape[1], job_tfidf.shape[1])  # both should be 50000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10853be3-2558-4e49-92c1-2d0d96d87a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
